{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sandbox - POI Version of lacounty_covid.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMtRDeC8+FqR0mx0x4Cu8Lm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JAy2UNWLSx5C"},"source":["# We're now making the full POI Version of lacounty_covid.json\n"]},{"cell_type":"markdown","metadata":{"id":"DAQe0YWSAyr-"},"source":["## Connect to Drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MwNnKAxZRt1a","executionInfo":{"status":"ok","timestamp":1608869356641,"user_tz":480,"elapsed":1202,"user":{"displayName":"Francisco Avalos Jr.","photoUrl":"","userId":"05379740764794418234"}},"outputId":"d0ed7ed0-d3b7-484a-a520-e29d45401e04"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8jR1_4vgRzW_"},"source":["import glob\n","import re\n","import os\n","import pandas as pd\n","import json\n","\n","from datetime import date, datetime, timedelta\n","\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.max_rows', 500)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lFWB0n0XSGQ2"},"source":["os.chdir('/content/drive/My Drive/safegraph_data/LA_Covid_Cases_Files')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQrupVWxA2Pi"},"source":["## Functions"]},{"cell_type":"code","metadata":{"id":"o71FBHntSH0L"},"source":["def clean_education_file(df):\n","  \"\"\"\n","  Input: dataframe\n","  Output: dataframe\n","  Function returns a dataframe for education section with total Covid-19 cases\n","  \"\"\"\n","  df['total_cases'] = df['total_confirmed_staff'] + df['total_confirmed_students']\n","  return df.loc[:, ['location_name', 'total_cases']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pRrEzv-SSMgF"},"source":["def clean_residual_congregate_and_acute_file(df):\n","  \"\"\"\n","  Input: dataframe\n","  Output: dataframe\n","  Function returns a dataframe for residual congragate and acute care settings section with total Covid-19 cases\n","  \"\"\"\n","  df['total_cases'] = df['number_of_confirmed_staff'] + df['number_of_confirmed_residents']\n","  return df.loc[:, ['location_name', 'total_cases']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XORNSaalSOwN"},"source":["def clean_non_residential_settings_file(df):\n","  \"\"\"\n","  Input: dataframe\n","  Output: dataframe\n","  Function returns a dataframe for non residential settings section with total Covid-19 cases\n","  \"\"\"\n","  if 'total_confirmed_non_staff' in df.columns:\n","    df['total_cases'] = df['total_confirmed_staff'] + df['total_confirmed_non_staff']\n","  elif 'total_confirmed_non_staff' not in df.columns:\n","    df['total_cases'] = df['total_confirmed_staff']\n","  return df.loc[:, ['location_name', 'total_cases']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_JQc_n-sSQ7c"},"source":["def get_file_list():\n","  \"\"\"\n","  Input: Nothing \n","  Output: list\n","  Function returns list of csv files from the targetted path\n","  \"\"\"\n","  files_list = []\n","  for file in glob.glob(\"*.csv\"):\n","    files_list.append(file)\n","  return files_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vDwkmnPjSTDe"},"source":["def get_dates(file_list):\n","  \"\"\"\n","  Input: List\n","  Output: List\n","  Function returns a list of unique dates from the input list\n","  \"\"\"\n","  dates = []\n","  for i in file_list:\n","    dates.append(i.split('_')[0:4][0])\n","  dates = set(dates)\n","  dates = list(dates)\n","  return dates"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V41r05gpwynr"},"source":["#### Function created post usc-research meet up\n","\n","def get_non_cumulative_poi(df):\n","  \"\"\"\n","  Input: dataframe\n","  Output: list\n","  Function returns a list of POIs that are not monotonically increasing\n","  \"\"\"\n","  dip_index_list = []\n","  \n","  for poi_name in df['location_name'].unique():\n","    dates = df[df['location_name']==f'{poi_name}'].sort_values('date')\n","    y = df[df['location_name']==f'{poi_name}'].sort_values('date')\n","    dates = dates['date']\n","    result = y['total_cases'].is_monotonic\n","\n","    if not result:\n","      dip_index_list.append(poi_name)\n","  \n","  return dip_index_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8yQpwiVb0Wu"},"source":["#### Function created post usc-research meet up\n","\n","def make_monotonic(poi_df, max_value):\n","  \n","  for i in poi_df['total_cases']:\n","    if (i == max_value):\n","      max_index = poi_df[poi_df['total_cases']==max_value].index.astype(int)[0]\n","      poi_df.loc[max_index+1:, ['total_cases']] = max_value\n","  return poi_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OUy-pwXhmGSK"},"source":["#### Function created post usc-research meet up\n","\n","def ensure_monotonicity(poi_vec):\n","  \n","  values_df = pd.DataFrame([i for i in poi_vec['total_cases'].values])\n","  values_df.columns = ['total_cases']\n","  counter = 0\n","  loops = values_df.shape[0]\n","\n","  while counter < loops:\n","    if counter+1 >= loops:\n","      break;\n","    if (values_df.loc[counter].values > values_df.loc[counter+1].values):\n","      values_df.loc[counter+1, ['total_cases']] = values_df.loc[counter, ['total_cases']]\n","    counter+=1\n","  \n","  output_df = pd.concat([poi_vec.loc[:, ['date', 'location_name']], values_df], axis=1).sort_values('date', ascending=True)\n","\n","  return output_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cloXmmW9ZevW"},"source":["#### Function created post usc-research meet up\n","\n","def make_poi_cumulative(non_cumulative_poi_df, total_poi_df):\n","  \n","  col_names = ['date', 'location_name', 'total_cases']\n","  output_df = pd.DataFrame(columns=col_names)\n","\n","  for i in range(0, len(non_cumulative_poi_df)):\n","    poi_name = non_cumulative_poi_df[i]\n","    poi_vec = total_poi_df[total_poi_df['location_name']==f'{poi_name}'].sort_values('date')\n","    max_value = poi_vec[poi_vec['location_name']==f'{poi_name}']['total_cases'].max()\n","    poi_vec = poi_vec.reset_index(drop=True)\n","\n","    temp_df = make_monotonic(poi_df=poi_vec, max_value=max_value)\n","    temp_df = ensure_monotonicity(temp_df)\n","\n","    output_df = output_df.append(temp_df, ignore_index=True)\n","  \n","  return output_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-ophzmDuppI"},"source":["def expand_missing_POI_data(POI_name, df):\n","  \"\"\"\n","  Input: string, dataframe \n","  Output: dataframe or null\n","  Function accepts the point of interest and the dataframe that contains all the\n","  points of interest. It then determines whether the POI given has been discontinued \n","  from showing through the most recent date in the input dataframe. If it has \n","  been discontinued, then the latest data for the given POI is copied to fill\n","  in those missing dates until it has data through the most recent date.\n","  \"\"\"\n","  \n","  sub_df = df[df['location_name']==POI_name]\n","  first_poi_day = sub_df.sort_values('date')['date'].min()\n","  last_poi_day = sub_df.sort_values('date')['date'].max()\n","  today=date.today() - timedelta(days=1) # recent data is 1 day behind... \n","  todays_date = today.strftime('%m-%d-%Y')\n","  needed_days = datetime.strptime(todays_date, '%m-%d-%Y').date() - datetime.strptime(last_poi_day, '%m-%d-%Y').date()\n","  needed_days = needed_days.days\n","\n","  date_list = []\n","  last_day_change = datetime.strptime(last_poi_day, '%m-%d-%Y').date()\n","\n","  if last_poi_day != todays_date:\n","    last_row = sub_df[sub_df['date']==last_poi_day].copy(deep=True)\n","    last_row_info=last_row.loc[:, ['location_name', 'total_cases']]\n","    df = pd.DataFrame()\n","    for i in range(0, needed_days):\n","      next_day = last_day_change+timedelta(i+1)\n","      next_day = next_day.strftime('%m-%d-%Y')\n","      date_list.append(next_day)\n","    df = df.append([last_row_info]*needed_days, ignore_index=True)\n","    date_list = pd.DataFrame(date_list)\n","    remaining_data_df = pd.concat([date_list, df], axis=1)\n","    remaining_data_df.columns = ['date', 'location_name', 'total_cases']\n","    \n","    return remaining_data_df\n","  elif last_poi_day == todays_date:\n","    return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPrPGomJSVRk"},"source":["files_list = get_file_list()\n","dates = get_dates(files_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8KVUpf11SXNp"},"source":["year = datetime.now()\n","year = year.strftime(\"%Y\")\n","\n","day_df = pd.DataFrame(columns=['date', 'location_name', 'total_cases'])\n","\n","for day in dates:\n","  day_data = pd.DataFrame(columns=['location_name', 'total_cases'])\n","  for f in files_list:\n","    if f.split('_')[0:4][0] == day:\n","      if re.search('.Non-Residential.', f):\n","        f_data = pd.read_csv(f)\n","        get_non_res_data = clean_non_residential_settings_file(f_data)\n","        day_data = day_data.append(get_non_res_data)\n","      if re.search('.Educational.', f):\n","        f_data = pd.read_csv(f)\n","        get_educ_data = clean_education_file(f_data)\n","        day_data = day_data.append(get_educ_data)\n","      if re.search('.Residual_Congregate.', f):\n","        f_data = pd.read_csv(f)\n","        get_residual_data = clean_residual_congregate_and_acute_file(f_data)\n","        day_data = day_data.append(get_residual_data)\n","  day_data['date'] = day[0]+day[1]+'-'+day[2]+day[3]+f'-{year}'\n","  ready_day_data = day_data.reindex(columns = ['date', 'location_name', 'total_cases'])\n","  day_df = day_df.append(ready_day_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0A91IommZIA"},"source":["#### Per usc-research meet up: Ensure each POI is cumulative\n"]},{"cell_type":"code","metadata":{"id":"I356mCKMt3F1"},"source":["# obtain Non-Monotonic POIs\n","non_cumulative_poi_df = get_non_cumulative_poi(day_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1ST77dkt_er"},"source":["# Separate Cumulative and Non-Cumulative POIs\n","cumulative_poi_df = day_df[~day_df.location_name.isin(non_cumulative_poi_df)].sort_values(['location_name', 'date'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GMOedX6xuCI3"},"source":["# Make non-cumulative pois monotonic\n","corrected_poi_df = make_poi_cumulative(non_cumulative_poi_df = non_cumulative_poi_df, total_poi_df = day_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"22xrLkHtmY0X"},"source":["# Produce final dataframe \n","final_poi_df = pd.concat([cumulative_poi_df, corrected_poi_df], axis=0).sort_values(['location_name', 'date'])\n","\n","del non_cumulative_poi_df, cumulative_poi_df, corrected_poi_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3WSA73aMxFwZ"},"source":["#### Fix POIs with missing data"]},{"cell_type":"code","metadata":{"id":"6aP2EKNzx3cX"},"source":["pois = final_poi_df['location_name'].unique()\n","pois = list(pois)\n","\n","poi_names_missing_df = []\n","\n","for poi in pois:\n","  poi = str(poi)\n","  result_df = expand_missing_POI_data(POI_name = poi, df=final_poi_df)\n","  # print(result_df)\n","  if result_df is not 0:\n","    # print(result_df)\n","    # track which pois need updating\n","    poi_names_missing_df.append(poi)\n","    # extract the existing poi data\n","    poi_existing_df = final_poi_df[final_poi_df['location_name']==poi]\n","    # drop the extracted data from main df, to prevent duplicating \n","    final_poi_df = final_poi_df.drop(final_poi_df[final_poi_df['location_name']==poi].index)\n","    # merge the extracted data with the filling data\n","    new_entry_df = pd.concat([poi_existing_df, result_df]).reset_index(drop=True)\n","    # add this merged data to the main df\n","    final_poi_df = final_poi_df.append(new_entry_df, ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fF9Zc9YqmowE"},"source":["#### Monotonicity"]},{"cell_type":"code","metadata":{"id":"9dsWX4kdSoEg"},"source":["\n","final_poi_df = final_poi_df.reset_index(drop=True)\n","\n","test_dict = {}\n","count = 16\n","\n","for i in set(final_poi_df['date']):\n","  day_list = []\n","  for x in range(0, len(final_poi_df)):\n","    if final_poi_df.loc[x, ['date']].values == i:\n","      day_list.append([final_poi_df.loc[x, ['location_name', 'total_cases']].values[0], \n","                       final_poi_df.loc[x, ['location_name', 'total_cases']].values[1]])\n","                      #  str(final_poi_df.loc[x, ['location_name', 'total_cases']].values[1])])\n","  to_string = str(count)\n","  test_dict[to_string] = day_list\n","  count+=1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bv1L8eISCPiB"},"source":["## Export data and save"]},{"cell_type":"code","metadata":{"id":"KijhlBi7StG1"},"source":["os.chdir('/content/drive/My Drive/safegraph_data/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0o_ykpXaTW1y"},"source":["with open(\"lacounty_covid_poi.json\", \"w\") as write_file:\n","  json.dump(test_dict, write_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JdwroakRxSI2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TwoNIYBjtfpL"},"source":["## We're now making the a single POI Version of lacounty_covid.json for testing purposes\n"]},{"cell_type":"code","metadata":{"id":"Lr6FfqNwtKuy"},"source":["# single_poi_test = {}\n","# count = 16\n","# POI_sample = \"Temple Park Convalescent Hospital\"\n","\n","# for i in final_poi_df['date'].unique():\n","#   df = final_poi_df[final_poi_df['date']==i]\n","#   df = df.reset_index(drop=True)\n","#   length = df[df['date']==i].shape[0]\n","\n","#   for row in range(0, length):\n","#     item = df.loc[row, :].values\n","#     if (re.search(POI_sample, item[1])):\n","#       to_string = str(count)\n","#       single_poi_test[to_string] = [[item[1], str(item[2])]]\n","#       count+=1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W7AY9YmuPi2G"},"source":["# os.chdir('/content/drive/My Drive/safegraph_data/SINGLE_POI_SAMPLE')\n","\n","# with open('single_poi.json', 'w') as outfile:\n","#   json.dump(single_poi_test, outfile)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1heYmI5aTJV"},"source":["# # final_poi_df.sort_values('total_cases').tail(500)\n","# # final_poi_df.groupby(['location_name']).sum()\n","# # final_poi_df.loc[7800:8000, :] # Holiday Manor Care Center\n","# len(final_poi_df)\n","# # poi_names_missing_df\n","# # final_poi_df[final_poi_df['location_name']=='Astoria Nursing and Rehab Center']"],"execution_count":null,"outputs":[]}]}